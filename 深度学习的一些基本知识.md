---
title: 深度学习的一些基本知识
date: 2017-03-08 00:00:01
categories:
- Deep_Learning
tags:
- deep_Learning
description: ...
---

### CNN
* Deconvolution or Transposed Convolution
  * http://blog.csdn.net/hjimce/article/details/50544370
  * https://www.zhihu.com/question/43609045
* fully-connected layer
  * http://blog.cvmarcher.com/posts/2015/05/17/cnn-trick/
* Batch Normarlization
  * http://blog.csdn.net/elaine_bao/article/details/50890491
* Dropout layer
  * https://zhuanlan.zhihu.com/p/23178423
  
### Training
* IoU
  * Intersection over Union IoU的值定义：Region Proposal与Ground Truth的窗口的交集比并集的比值，如果IoU低于0.5，那么相当于目标还是没有检测到
  * 在《The Cityscapes Dataset for Semantic Urban Scene Understanding》中有一个更加详细的定义
* epoch(轮)、batch-size、iter_size等
  * epoch: 1个epoch就是所有训练样例全部训练一次
  * batch-size(mini-batch): 由于把所有的样例进行一次训练，然后求偏导数计算量太大，所以采用随机梯度下降算法SGD.SGD每次采样batch-size个样例进行训练，然后进行一次Forward和BP操作.
  * iterations: 每一次迭代就是一次权重更新，也就是一个mini-batch训练完成.
  * 因此，有**总的训练样本数是N,则一个epoch训练的样本数就是N,一个batch-size大小为n，则iterations = N/n**
  * 在solver.prototxt中有时候还有一个参数是iter_size, batch-size是在train.prototxt中的，每次循环都会以batch_size大小计算梯度和loss，最后再取iter_size次的平均.可以看成iter_size*batch_size次更新一次参数.这样的好处是比一次使用大的batch_size要节省存储.
